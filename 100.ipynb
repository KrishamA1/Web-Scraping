{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv: provides functionality to read from and write to CSV files.\n",
    "# requests: used to send HTTP requests and handle responses, which is essential for web scraping.\n",
    "# BeautifulSoup: Part of the bs4 package, it parses HTML and XML documents (allowing easy extraction of data from web pages)\n",
    "# urllib.parse: Specifically, urlparse and urljoin functions (to manipulate URLs)\n",
    "# pandas: A powerful data manipulation and analysis library (though it appears not to be used in the provided code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_social_media_url(url):\n",
    "    social_media_domains = ['twitter.com','facebook.com','whatsapp.com','instagram.com']\n",
    "    for domain in social_media_domains:\n",
    "        if domain in url:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This function checks if a given URL belongs to a social media domain (e.g., Twitter, Facebook). It helps filter out links to social media sites, which are not relevant to your scraping goals as they might require authenticaion and may involve legal permission to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extaract_all_links(url):\n",
    "    try:\n",
    "        # send a GET request to URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        #check if the request was sucessful(status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page.\n",
    "            soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "            # Extract all links using BeautifulSoup Method\n",
    "            links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "            # Convert relative URLS to Absolute URLS.\n",
    "            links = [urljoin(url, link) for link in links]\n",
    "\n",
    "            return links\n",
    "        else:\n",
    "            print(f\"Failed to retrive data from '{url}'. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to {url}: {e}\")\n",
    "        return [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Try-Except Block: Handles any request exceptions that might occur (e.g., network issues and relevant info extraction issues).\n",
    " GET Request: Sends an HTTP GET request to fetch the HTML content of the provided URL.\n",
    " Status Code Check: Verifies if the request was successful (sucess status code 200).\n",
    " BeautifulSoup Parsing: Parses the HTML content to make it navigable.\n",
    "# Link Extraction: \n",
    "Finds all anchor tags (<a>) with href attributes and extracts the links.\n",
    "# URL Conversion: \n",
    "Converts relative URLs to absolute URLs using urljoin.\n",
    "\n",
    " Overall website URL: https://thehimalayantimes.com/world/modis-party-has-comfortable-lead-over-main-rival-in-early-vote-counting-from-indias-election\n",
    "\n",
    "\n",
    " In this:\n",
    "\n",
    "# Relative Url:\n",
    "modis-party-has-comfortable-lead-over-main-rival-in-early-vote-counting-from-indias-election\n",
    " (as providing the relative URL cannot function properly as we want.)\n",
    "\n",
    "# Absolute Url:  \n",
    "https://thehimalayantimes.com/world/\n",
    " (absolute can function independently but the info we eant cannot be retrived using only the absolute URL.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_links_to_csv(links, csv_filename):\n",
    "    with open(csv_filename,'w', newline= '') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv.writer.writerow(['Link'])   # Write Header.\n",
    "\n",
    "        for link in links:\n",
    "            csv_writer.writerow([link])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open File: Opens (or creates) a CSV file for writing. newline='' ensures correct line endings on all platforms.\n",
    "# CSV Writer: Creates a writer object to write rows to the CSV file.\n",
    "# Write Header: Writes a header row with the column name \"Link\".\n",
    "# Write Links: Iterates through the list of links and writes each one as a new row in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_specific_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Skip extraction for social media URLs.\n",
    "        if is_social_media_url(url):\n",
    "            print(f\"Skipping {url} because it's a social media link.\")\n",
    "            return None,None,None,None,None,None # Include the original link as None.\n",
    "        \n",
    "        heading_element = soup.find('h1',{'style': 'margin-bottom: 0.1rem;'})\n",
    "        author_element = soup.find('h5',class_ = 'text-capitalize')\n",
    "        publication_date_element = soup.find('div',class_ = 'updated-time')\n",
    "        content_container = soup.find('div',class_ = 'subscribe--wrapperx')\n",
    "\n",
    "        # Determine Category\n",
    "        url_parts = urlparse(url).path.split('/') \n",
    "        category  = next((part for part in url_parts if part), 'Category not found')\n",
    "\n",
    "        heading =  heading_element.text.strip() if heading_element else 'Heading not Found'\n",
    "        author = author_element.text.strip() if author_element else 'Author not Found'\n",
    "        publication_date_raw = publication_date_element.text.strip() if publication_date_element else 'Date not found'\n",
    "        publication_date = publication_date_raw.replace('Published at :', '').strip()\n",
    "        content = content_container.get_text(separator=' ', strip=True) if content_container else 'Content not found'\n",
    "\n",
    "        return heading, author, publication_date, content, url, category\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to retrieve content from {url}. Error: {e}\")\n",
    "        return None, None, None, None, url, None  # Include the original link and None for category\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET Request and Error Handling: Similar to the previous function, it fetches the page content and raises an exception for bad responses.\n",
    "# Check for Social Media: Skips processing if the URL is identified as a social media link.\n",
    "# HTML Parsing: Uses BeautifulSoup to parse the HTML content.\n",
    "# Element Extraction: Extracts specific elements:\n",
    "# heading_element: Finds the article heading.\n",
    "# author_element: Finds the author's name.\n",
    "# publication_date_element: Finds the publication date.\n",
    "# content_container: Finds the main content of the article.\n",
    "# Category Determination: Parses the URL path to determine the article category.\n",
    "# Text Extraction: Retrieves and cleans the text content from the extracted elements.\n",
    "# Return Data: Returns the extracted data as a tuple."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
